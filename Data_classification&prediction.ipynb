{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c289568f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "import requests\n",
    "from pymystem3 import Mystem\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import precision_score , recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76f9b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbad28b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c16c23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc715ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigdata = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c65e5c",
   "metadata": {},
   "source": [
    "## Выгрузка данных из 3ех новостных групп в вконтакте"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a30a85",
   "metadata": {},
   "source": [
    " За тематику текстов для классификации выбрал следущее:\n",
    " Новости про covid, Спам в новостях, Непосредственно новости.\n",
    "\n",
    " Сами данные я решил сам разметить, а не выбирая сразу данные из конкретных тематик,\n",
    " например из отдельных групп с новостями про : науку, экономику, здоровье, животных и т.д.\n",
    "\n",
    " Я выбрал группы, которые выкладывают новости об всем что угодно, и пытаюсь сам их разметить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9c449f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://oauth.vk.com/authorize?client_id=7990988&display=page&scope=wall&response_type=token&v=5.131"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dc44c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "token ='34e0cbf531055e09361376487d0bd9704478ca1072a730091388deaf4a62c22e8f2003c8e47b7cf6d481a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccc482f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://vk.com/toporcc\n",
    "# https://vk.com/plohie_novosti_mc\n",
    "# https://vk.com/lentach\n",
    "IDS = 'toporcc','plohie_novosti_mc','lentach'\n",
    "\n",
    "for ID in IDS:\n",
    "    json_response = requests.get(('https://api.vk.com/method/wall.get?domain={}&count=100&v=5.103&access_token=' + token).\\\n",
    "                                 format(ID)).json()\n",
    "    if json_response.get('error'):\n",
    "        print(json_response.get('error'))\n",
    "    else:\n",
    "        for item in json_response['response']['items']:\n",
    "            bigdata.append(item['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ad11a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bigdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9fae7b",
   "metadata": {},
   "source": [
    "## Разметка выгруженных данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964103f3",
   "metadata": {},
   "source": [
    " Данный способ имеет большую погрешность в разметке, потому, что 'ключевые слова' из различных классов могут быть употреблены\n",
    " \n",
    " В другом классе, но при этом этот алгоритм разметки отнесет его в класс по 'ключевому слову'\n",
    " \n",
    " Лучше смотреть на смысловую часть предложений\\кусков текста используя готовые методы из NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "189183b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spamwords = ['кредит','заем','займ','подарить','промокод','активация','выгода','халявный','резюме','smartzaem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ccd0948",
   "metadata": {},
   "outputs": [],
   "source": [
    "covidwords = ['коронавирус','вирус','заболеть','болеть','больной','заражать','пандемия','coronavirus','вакцинация']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a7d5270",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ba58c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_sentence(text):\n",
    "    lemmas=m.lemmatize(text)\n",
    "    return \"\".join(lemmas).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0990807e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam = []\n",
    "covid = []\n",
    "news = []\n",
    "\n",
    "testset = []\n",
    "dataset = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e4cb15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~20% из данных будут тестом\n",
    "\n",
    "# Русские стоп слова, их нужно отбросить (союзы,предлоги)\n",
    "stop_words = stopwords.words(\"russian\")\n",
    "    \n",
    "for data in bigdata:\n",
    "    \n",
    "    if np.random.rand(1)>0.2:\n",
    "        \n",
    "        # Приведение слов к их инфинитиву\n",
    "        doc = lemmatize_sentence(data)\n",
    "        # Разбиение на слова предложения\n",
    "        tokens_data=re.split(r'[-\\s\\n\\t-:\\[\\]<>\\'.,;!?()\" ]+',doc)\n",
    "        \n",
    "        words_data = []\n",
    "        \n",
    "        # Получение выборки\n",
    "        for token_data in tokens_data:\n",
    "            if ((token_data not in stop_words) and (len(token_data)>3)):\n",
    "                \n",
    "                words_data.append(token_data)\n",
    "                \n",
    "                \n",
    "        # Разметка данных\n",
    "        c=0\n",
    "        \n",
    "        for spamword in spamwords:\n",
    "            if spamword in words_data:\n",
    "                spam.append(words_data)\n",
    "                #dataset.append([words_data,0])\n",
    "                c+=1\n",
    "                break\n",
    "            \n",
    "        if c==0:\n",
    "            for covidword in covidwords:\n",
    "                if covidword in words_data:\n",
    "                    covid.append(words_data)\n",
    "                    #dataset.append([words_data,1])\n",
    "                    c+=1\n",
    "                    break\n",
    "    \n",
    "        if c==0:\n",
    "            news.append(words_data)\n",
    "            #dataset.append([words_data,2])\n",
    "            \n",
    "    else:\n",
    "\n",
    "        # Приведение слов к их инфинитиву\n",
    "        doc = lemmatize_sentence(data)\n",
    "        # Разбиение на слова предложения\n",
    "        tokens_data=re.split(r'[-\\s\\n\\t-:\\[\\]<>\\'.,;!?()\" ]+',doc)\n",
    "        \n",
    "        # Получение выборки \n",
    "        words_data = []\n",
    "        \n",
    "        for token_data in tokens_data:\n",
    "            if ((token_data not in stop_words) and (len(token_data)>3)):\n",
    "                words_data.append(token_data)\n",
    "        \n",
    "        # Заполнение тестового датасета\n",
    "        testset.append(words_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c8dcf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "637f34c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upd_dataset(array,num_class):\n",
    "    for data in array:\n",
    "        dataset.append([data,num_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b41dfeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "upd_dataset(spam,0)\n",
    "upd_dataset(covid,1)\n",
    "upd_dataset(news,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2718ddbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Объем тестовой выборки\n",
    "len(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fa137ee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "232"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Объем тренировочной выборки\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bca88a",
   "metadata": {},
   "source": [
    "## Обучение (Наивный байесовский классификатор)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62700610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit (dataset, alpha):\n",
    "    classes, freq, tot_in_cl, total = {}, {}, {}, set()\n",
    "    for features, label in dataset:\n",
    "        if label not in classes:\n",
    "            classes[label] = 0\n",
    "            tot_in_cl[label] = 0\n",
    "        classes[label] += 1\n",
    "        for feature in features:\n",
    "            if (feature,label) not in freq:\n",
    "                freq[(feature,label)] = 0\n",
    "            freq[(feature,label)] += 1\n",
    "            tot_in_cl[label] += 1\n",
    "            total.add(feature)\n",
    "            \n",
    "            \n",
    "    # Перевод на язык вероятностей\n",
    "    # Частоты слов\n",
    "    for feature,label in freq:\n",
    "        freq[(feature,label)] = ((alpha + freq[(feature,label)] ) /  (alpha * len(total) + tot_in_cl[label]) )\n",
    "        \n",
    "    # Частоты классов    \n",
    "    for cl in classes:\n",
    "        classes[cl] /= len(dataset)\n",
    "        \n",
    "    \n",
    "    return alpha,classes, freq, tot_in_cl, len(total)\n",
    "\n",
    "def predict(classifier , features):\n",
    "    alpha, classes, freq, tot_in_cl, len_total = classifier\n",
    "    # Лямбда функция\n",
    "    return max(classes.keys(),\n",
    "              key = lambda cl:\n",
    "              np.log10(classes[cl]) + \n",
    "                sum(np.log10(freq.get((feature,cl), alpha/(alpha*len_total+tot_in_cl[cl])) )  \\\n",
    "                  for feature in features ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "212c981b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fit(dataset,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08b33816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_dict.pkl']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Сохранение обученной модели\n",
    "#joblib.dump(model,'model_dict.pkl')\n",
    "\n",
    "# Использование обученной модели\n",
    "#model = joblib.load('model_dict.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccc8aed",
   "metadata": {},
   "source": [
    "## Разметка тестовых данных (y) ,  и прогнозирование модели (y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a807095",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b2b70fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for test in testset:\n",
    "    c = 0\n",
    "    for spamword in spamwords:\n",
    "        if spamword in test:\n",
    "            y.append(0)\n",
    "            c+=1\n",
    "            break\n",
    "            \n",
    "    if c==0:\n",
    "        for covidword in covidwords:\n",
    "            if covidword in test:\n",
    "                y.append(1)\n",
    "                c+=1\n",
    "                break\n",
    "    \n",
    "    if c==0:\n",
    "        y.append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "494a852e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c80a48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for testdata in testset:\n",
    "    y_pred.append(predict(model,testdata))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3d425f",
   "metadata": {},
   "source": [
    "# Подсчет F меры, так как классы не сбалансированы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d8fc82dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_score = precision_score(y,y_pred,average = 'micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "017bb428",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_score = recall_score(y, y_pred,average = 'micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "da83d6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "F = pr_score*rec_score/(pr_score+rec_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9c991791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3382352941176471"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
